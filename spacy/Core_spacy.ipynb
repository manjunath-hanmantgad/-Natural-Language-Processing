{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b43b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pipeline brekdown\n",
    "\n",
    "import spacy\n",
    "#--\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "#spacy.load() returned a Language class instance, nlp. The Language class is the text processing pipeline.\n",
    "\n",
    "#--\n",
    "doc = nlp(\"I went there.\")\n",
    "#After that, we applied nlp on the sample sentence I went there and got a Doc class instance, doc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cafe92",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f54b29",
   "metadata": {},
   "source": [
    "It is first step in NLP Pipline.\n",
    "Tokenization simply means splitting the sentence into its tokens. A token is a unit of semantics. You can think of a token as the smallest meaningful part of a piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b3a91",
   "metadata": {},
   "source": [
    "Tokens can be words, numbers, punctuation, currency symbols, and any other meaningful symbols that are the building blocks of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10bdcf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'apple', 'and', 'Apple', '.']\n"
     ]
    }
   ],
   "source": [
    "# process of tokenization \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = nlp(\"I like apple and Apple.\")\n",
    "\n",
    "print([token.text for token in doc])\n",
    "#print([i.text for i in doc])\n",
    "\n",
    "#for i in doc:\n",
    "    #print(i.text)\n",
    "    \n",
    "# spaCy generates the Token objects implicitly when we created the Doc object.\n",
    "# that is why use token and not anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c8898",
   "metadata": {},
   "source": [
    "Most domains that you'll process have characteristic words and phrases that need custom tokenization rules.\n",
    "\n",
    "When we work with a specific domain such as medicine, insurance, or finance, we often come across words, abbreviations, and entities that needs special attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd065120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemme', 'that']\n"
     ]
    }
   ],
   "source": [
    "# invoke tokenizer's custom class\n",
    "\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = nlp(\"lemme that\")\n",
    "\n",
    "print([i.text for i in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lem', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "special_case = [{ORTH:\"lem\"} , {ORTH:\"me\"}]\n",
    "# ORTH is for orthography which means text\n",
    "nlp.tokenizer.add_special_case(\"lemme\" , special_case)\n",
    "# We defined a special case, where the word lemme should tokenize as two tokens, lem and me.\n",
    "# and then added the rule to the nlp object's tokenizer.\n",
    "\n",
    "print([i.text for i in nlp(\"lemme that\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c34a7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lem', 'me', 'that', '!']\n"
     ]
    }
   ],
   "source": [
    "# similarly\n",
    "\n",
    "print([i.text for i in nlp(\"lemme that!\")])\n",
    "# here it will tokenize ! also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a8c86",
   "metadata": {},
   "source": [
    "Debugging the tokenizer using :  nlp.tokenizer.explain(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376cd475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let \t SPECIAL-1\n",
      "'s \t SPECIAL-2\n",
      "go \t TOKEN\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"Let's go!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# the explainer !\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "\n",
    "for t in tok_exp:\n",
    "    #print(t[0], \"\\t\", t[1])\n",
    "    #print(t[1], \"\\t\", t[2]) --> out of range!\n",
    "\n",
    "    print(t[1], \"\\t\", t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4dfb51",
   "metadata": {},
   "source": [
    "#### Tokenizing sentences!!\n",
    "\n",
    "Spacy uses dependency parser to tokenize sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dccaed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I flied to N.Y yesterday.\n",
      "It was around 5 pm.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32db94",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemma is base form of token.\n",
    "e.g eating - eat \n",
    "sitting - sit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc0bc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "went go\n",
      "there there\n",
      "and and\n",
      "will will\n",
      "eats eat\n",
      "an an\n",
      "ice ice\n",
      "- -\n",
      "creams cream\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#doc = nlp(\"I go there and will eat an ice-cream.\")\n",
    "\n",
    "doc = nlp(\"I went there and will eats an ice-creams.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text , token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411c3f4",
   "metadata": {},
   "source": [
    "Playing with token classes.\n",
    "\n",
    "-token.text\n",
    "\n",
    "-token.text_with_ws\n",
    "\n",
    "-token.i\n",
    "\n",
    "-token.idx\n",
    "\n",
    "-token.doc\n",
    "\n",
    "-token.sent\n",
    "\n",
    "-token.is_sent_start\n",
    "\n",
    "-token.ent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f277830e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Hello doctor!\")\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf962421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello doctor!\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# token.text\n",
    "# -- doc = nlp(\"Hello doctor!\")\n",
    "print(doc.text)\n",
    "print(doc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aab4e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token.text_with_ws gives whitespace if present in sentence\n",
    "\n",
    "doc[0].text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aecbc159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# length of token\n",
    "print(len(doc))\n",
    "print(len(doc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f9bde0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token.i gives index of token\n",
    "\n",
    "token = doc[0]\n",
    "token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5e0dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token.idx gives poistion \n",
    "doc[0].idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4500e01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].idx\n",
    "# returns 6 because doctor which is at doc[1] starts at 6th poistion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cbb1402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tim Cook,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using entities\n",
    "\n",
    "doc = nlp(\"Tim Cook is CEO\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93f236e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].ent_type_\n",
    "# if result is ' ' that means it is not an entity\n",
    "# eg is and CEO are not entities\n",
    "# however TIM and COOK are entities of type PERSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b48ffe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_get_array_attrs',\n",
       " '_py_tokens',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e1573",
   "metadata": {},
   "source": [
    "#### Span Object\n",
    "Span objects represent phrases or segments of the text. Technically, a Span has to be a contiguous sequence of tokens. We usually don't initialize Span objects, rather we slice a Doc object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef94b189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ". He is from USA."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Tim Cook is CEO. He is from USA.\")\n",
    "doc[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcc95cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tim Cook is CEO"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec67004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CEO. He is from"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f4b8663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "371d5aa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E040] Attempt to access token at 10, max length 10.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\0023ND~1\\AppData\\Local\\Temp/ipykernel_11588/2652168159.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\0023nd744\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\0023nd744\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\spacy\\tokens\\token.pxd\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.cinit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: [E040] Attempt to access token at 10, max length 10."
     ]
    }
   ],
   "source": [
    "doc[10] # gives indexerror."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f566864",
   "metadata": {},
   "source": [
    "### Spacy commonly used features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda50d1b",
   "metadata": {},
   "source": [
    "token.lower_ returns the token in lowercase.\n",
    "\n",
    "is_alpha returns True if all the characters of the token are alphabetic letters.\n",
    "\n",
    "is_ascii returns True if all the characters of token are ASCII characters.\n",
    "\n",
    "is_digit returns True if all the characters of the token are numbers.\n",
    "\n",
    "is_punct returns True if the token is a punctuation mark.\n",
    "\n",
    "is_left_punct and is_right_punct return True if the token is a left punctuation mark or right punctuation mark, respectively.\n",
    "\n",
    "is_space returns True if the token is only whitespace characters.\n",
    "\n",
    "is_bracket returns True for bracket characters.\n",
    "\n",
    "is_quote returns True for quotation marks.\n",
    "\n",
    "\n",
    "is_currency returns True for currency symbols such as $ and €.\n",
    "\n",
    "like_url, like_num, and like_email are methods about the token shape and return True if the token looks like a URL, a number, or an email, respectively.\n",
    "\n",
    "is_oov and is_stop are semantic features, as opposed to the preceding shape features. is_oov returns True if the token is Out Of Vocabulary (OOV), that is, not in the Doc object's vocabulary. OOV words are unknown words to the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d8459",
   "metadata": {},
   "source": [
    "### token.shape_\n",
    "token.shape_ is an unusual feature – there is nothing similar in other NLP libraries. It returns a string that shows a token's orthographic features. Numbers are replaced with d, uppercase letters are replaced with X, and lowercase letters are replaced with x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ea5b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tim Xxx\n",
      "Cook Xxxx\n",
      "is xx\n",
      "Apple Xxxxx\n",
      "CEO XXX\n",
      "in xx\n",
      "2021 dddd\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# example of token.shape_\n",
    "\n",
    "doc = nlp(\"Tim Cook is Apple CEO in 2021.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text , token.shape_)\n",
    "    \n",
    "# so for every caps letter it prints X\n",
    "# and for small letter it prints x\n",
    "# for number it prints d so 123 becomes ddd\n",
    "# . remains as ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a20da",
   "metadata": {},
   "source": [
    "**is_stop**  is a feature that is frequently used by machine learning algorithms. Often, we filter words that do not carry much meaning, such as the, a, an, and, just, with, and so on. Such words are called stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed6fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
